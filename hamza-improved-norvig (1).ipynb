{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10854803,"sourceType":"datasetVersion","datasetId":6742221},{"sourceId":10854930,"sourceType":"datasetVersion","datasetId":6742313},{"sourceId":10855352,"sourceType":"datasetVersion","datasetId":6742608},{"sourceId":10859002,"sourceType":"datasetVersion","datasetId":6745400},{"sourceId":10859173,"sourceType":"datasetVersion","datasetId":6745528},{"sourceId":10913082,"sourceType":"datasetVersion","datasetId":6783854}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Context-sensitive Spelling Correction\n\nThe goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n\nSubmit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n\nUseful links:\n- [Norvig's solution](https://norvig.com/spell-correct.html)\n- [Norvig's dataset](https://norvig.com/big.txt)\n- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n\nGrading:\n- 60 points - Implement spelling correction\n- 20 points - Justify your decisions\n- 20 points - Evaluate on a test set\n","metadata":{"id":"DIgM6C9HYUhm"}},{"cell_type":"markdown","source":"## Implement context-sensitive spelling correction\n\nYour task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n\nThe best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n\nWhen solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are: \n\n- solving a problem of n-grams frequencies storing for a large corpus;\n- taking into account keyboard layout and associated misspellings;\n- efficiency improvement to make the solution faster;\n- ...\n\nPlease don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n\n##### IMPORTANT:  \nYour project should not be a mere code copy-paste from somewhere. You must provide:\n- Your implementation\n- Analysis of why the implemented approach is suggested\n- Improvements of the original approach that you have chosen to implement","metadata":{"id":"x-vb8yFOGRDF"}},{"cell_type":"code","source":"QWERTY_ADJACENCY = {\n    'q': 'wa', 'w': 'qase', 'e': 'wsdr', 'r': 'edft', 't': 'rfgy', 'y': 'tghu', 'u': 'yihj', \n    'i': 'uojk', 'o': 'ipkl', 'p': 'ol', 'a': 'qwsz', 's': 'awedxz', 'd': 'serfcx', 'f': 'drtgv', \n    'g': 'ftyhbv', 'h': 'gyujnb', 'j': 'huikmn', 'k': 'jiolm', 'l': 'kop', 'z': 'asx', 'x': 'zsdc', \n    'c': 'xdfv', 'v': 'cfgb', 'b': 'vghn', 'n': 'bhjm', 'm': 'njk'\n}\n\ndef keyboard_edits(word):\n    \"\"\"Generate candidate words by substituting letters with adjacent keyboard keys.\"\"\"\n    candidates = set()\n    for i, c in enumerate(word):\n        if c in QWERTY_ADJACENCY:\n            for adj in QWERTY_ADJACENCY[c]:\n                candidates.add(word[:i] + adj + word[i+1:])\n    return candidates\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T04:50:21.938735Z","iopub.execute_input":"2025-03-05T04:50:21.939006Z","iopub.status.idle":"2025-03-05T04:50:21.944366Z","shell.execute_reply.started":"2025-03-05T04:50:21.938986Z","shell.execute_reply":"2025-03-05T04:50:21.943626Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#############################################\n# USAGE\n# -----\n# cd ./data/traintest\n# python download_datafiles.py\n#############################################\n\n#taken from https://github.com/nsadawi/Download-Large-File-From-Google-Drive-Using-Python\n#taken from this StackOverflow answer: https://stackoverflow.com/a/39225039\nimport requests\nimport os\n\ndef download_file_from_google_drive(id, destination):\n    URL = \"https://docs.google.com/uc?export=download\"\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { 'id' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { 'id' : id, 'confirm' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, \"wb\") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n\ndef create_paths(path_: str):\n    if not os.path.exists(path_):\n        os.makedirs(path_)\n        print(f\"{path_} created\")\n    else:\n        print(f\"{path_} already exists\")\n\n\nif __name__==\"__main__\":\n\n    \"\"\"\n    All files from https://drive.google.com/drive/folders/1ejKSkiHNOlupxXVDMg67rPdqwowsTq1i?usp=sharing\n    will be downloaded to the current folder directory\n    \"\"\"\n\n    # ./\n\n    # download_file_from_google_drive('1ZlEQKf3HMMk66F7DGFPnh-PA2cbt5K0F', 'test.1blm')\n    # download_file_from_google_drive('1wZ6nrIYANNN3ZoHgacIg9P3UmHnBb9Wa', 'test.1blm.noise.prob')\n    # download_file_from_google_drive('1epwQQjmOZyZL1ptc9mcIFjnwS0vs7L46', 'test.1blm.noise.random')\n    # download_file_from_google_drive('1aT3mUfsNtTl51vc-V7kJZeflxZ4BMicD', 'test.1blm.noise.word')\n\n    download_file_from_google_drive('1QxVnFgp0pgEWmS-113SWEjT8tEhXCVF5', 'test.bea4k')\n    download_file_from_google_drive('1pnCU3OUSE0lNN1T6qY4WWhtHZsW3cg1c', 'test.bea4k.noise')\n\n    # download_file_from_google_drive('1eXrAPKzfU7E9EZNKMyyanuxL9NMpkvdv', 'test.bea20k')\n    # download_file_from_google_drive('178AWu05IzYFBOFYQ0lhkkBQaIACSJzAC', 'test.bea20k.noise')\n\n    # download_file_from_google_drive('10VtrEThrDIiuFJf0gj4LeGDdP-y-yR--', 'test.bea60k')\n    # download_file_from_google_drive('16AMIb6FVltgRR8xv8h7qacDUX8cOQK9d', 'test.bea60k.noise')\n\n    # download_file_from_google_drive('192g_5oJn4dro5QJ88Dd8-lN_xKE_lLf0', 'test.bea322')\n    # download_file_from_google_drive('1_hka2FOT4FrMvsV3d4Zfi9W8v3oFBRYc', 'test.bea322.noise')\n\n    # download_file_from_google_drive('1v0tRcNZctvVGqrmjlda_6dH8AfZUzFGO', 'test.bea4660')\n    # download_file_from_google_drive('1EmuKeNgBRzc760R0dSSuZ36xdikQRlIS', 'test.bea4660.noise')\n\n    # download_file_from_google_drive('1jHR2f3JwnskDphQcaTXr0hLlp60qJxUl', 'test.jfleg')\n    # download_file_from_google_drive('1sccH7dRhyctKAIQXBZEBmUWEiTN_-o6q', 'test.jfleg.noise')\n\n    # download_file_from_google_drive('1aWHIxu_BrZIeGRLhID3J_od6shXz3jUb', 'train.1blm')\n    # download_file_from_google_drive('16RYImD2esgGwc1nNt3Yf-WR5TU1yQyik', 'train.1blm.noise.prob')\n    # download_file_from_google_drive('11FMI2C-ouwaWesTLjfPCXmqeB6HUQHkK', 'train.1blm.noise.random')\n    # download_file_from_google_drive('1eRpWqSb7sIm3kgtkdfVTru9YKHSRRrdq', 'train.1blm.noise.word')\n    \n    # download_file_from_google_drive('1INTWXWO6i1Swthu5ln7REZjGvPFv2hyQ', 'train.bea40k')\n    # download_file_from_google_drive('1KTeL8oZ30fVI_QuCW879T-CgfeewvSk9', 'train.bea40k.noise')\n\n    # download_file_from_google_drive('1s6CQ6NlsstCLbLCSEZyP-uvNwx4UwzZT', 'train.moviereviews')\n    # download_file_from_google_drive('1xk3jyTkiVEWDsl-Abhc8XUXiIVp_WXCg', 'valid.moviereviews')\n\n\n    # # wo_context\n    # create_paths(\"./wo_context\")\n\n    # download_file_from_google_drive('1uNHQovF2Z0QPp27i2Sq5abVL_f56iNYK', './wo_context/aspell_big')\n    # download_file_from_google_drive('19eSfVnX-sIdUnaazEaUCsHsFt8p5qoXZ', './wo_context/aspell_big.noise')\n    \n    # download_file_from_google_drive('1XqHN1VnVnVnSR4-wF_iI6VUVPf9S_TP3', './wo_context/aspell_small')\n    # download_file_from_google_drive('1I9jhthL6y52h8uRuwcnRjhtRbNw3mX8V', './wo_context/aspell_small.noise')\n    \n    # download_file_from_google_drive('1ptBfh8UvbAUH7K1TIALDZM8CUL_Yhxty', './wo_context/combined_data')\n    # download_file_from_google_drive('1bSye_TITRUdO4CUIt9i1R2PkCdQD346h', './wo_context/combined_data.noise')\n    \n    # download_file_from_google_drive('1bj9zQqntrVRydBn-YHZ0BcT55Xf37jq-', './wo_context/homophones')\n    # download_file_from_google_drive('1rL_OdqQgr-kL6X_N94epxzpnIzj_L6y6', './wo_context/homophones.noise')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T04:50:23.518960Z","iopub.execute_input":"2025-03-05T04:50:23.519282Z","iopub.status.idle":"2025-03-05T04:50:29.243763Z","shell.execute_reply.started":"2025-03-05T04:50:23.519253Z","shell.execute_reply":"2025-03-05T04:50:29.242771Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from collections import Counter\n\ndef generate_ngrams(words, n):\n    return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n\ndef process_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        words = []\n        for line in file:\n            parts = line.strip().split('\\t')  # Split by tab\n            words.extend(parts[1:])  # Ignore the first column (frequency count)\n    \n    trigrams = generate_ngrams(words, 3)\n    fourgrams = generate_ngrams(words, 4)\n    \n    trigram_counts = Counter(trigrams)\n    fourgram_counts = Counter(fourgrams)\n    \n    print(\"Top Trigrams:\")\n    for trigram, count in trigram_counts.most_common(10):\n        print(trigram, count)\n    \n    print(\"\\nTop Fourgrams:\")\n    for fourgram, count in fourgram_counts.most_common(10):\n        print(fourgram, count)\n    return Counter(trigrams) ,Counter(trigrams)\n\n# Replace 'your_file.txt' with your actual file path\nTRIGRAMS , FOURGRAMS = process_file('/kaggle/input/fivegrams/fivegrams (2).txt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T04:50:29.244782Z","iopub.execute_input":"2025-03-05T04:50:29.245005Z","iopub.status.idle":"2025-03-05T04:50:41.174855Z","shell.execute_reply.started":"2025-03-05T04:50:29.244987Z","shell.execute_reply":"2025-03-05T04:50:41.174063Z"}},"outputs":[{"name":"stdout","text":"Top Trigrams:\n('one', 'of', 'the') 6223\n('a', 'lot', 'of') 6144\n('the', 'united', 'states') 5256\n('of', 'the', 'the') 4992\n('out', 'of', 'the') 3283\n('be', 'able', 'to') 3061\n('going', 'to', 'be') 2864\n('to', 'be', 'a') 2831\n('the', 'of', 'the') 2816\n('some', 'of', 'the') 2615\n\nTop Fourgrams:\n('in', 'the', 'united', 'states') 1299\n('of', 'the', 'of', 'the') 1055\n('of', 'the', 'to', 'the') 996\n('of', 'the', 'in', 'the') 963\n('of', 'the', 'and', 'the') 795\n('in', 'front', 'of', 'the') 715\n('as', 'well', 'as', 'the') 654\n('the', 'rest', 'of', 'the') 649\n('at', 'the', 'end', 'of') 601\n('of', 'the', 'on', 'the') 598\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import re\nfrom collections import Counter, defaultdict\nimport heapq\nimport nltk\nfrom nltk.corpus import gutenberg\n# Load a simple word frequency dataset (could be replaced with big.txt or N-gram data)\ndef load_words(filename='/kaggle/input/traindata/big.txt'):\n    with open(filename, 'r', encoding='utf-8') as file:\n        text = file.read().lower()\n    return Counter(re.findall(r'\\w+', text))\n\nWORDS = load_words()  # Word frequency from corpus\n\n# Load bigram frequencies (simplified here; ideally from N-gram dataset)\ndef generate_bigrams(filename='/kaggle/input/bigrams/bigrams (2).txt'):\n    bigrams = defaultdict(int)\n    with open(filename, 'r', encoding='ISO-8859-1') as file:\n        for line in file:\n            count,  w1, w2, = line.split()\n            bigrams[(w1.lower(), w2.lower())] = int(count)\n    return bigrams\n# def generate_bigrams():\n#     words = [w.lower() for w in gutenberg.words('austen-emma.txt') if w.isalpha()]\n#     bigrams = defaultdict(int)\n#     for i in range(len(words) - 1):\n#         bigrams[(words[i], words[i + 1])] += 1\n#     return bigrams\n\nBIGRAMS = generate_bigrams()\n# FIGRAMS = generate_bigrams(filename='/kaggle/input/fivegrams/fivegrams (2).txt')\n# BIGRAMS = load_bigrams()\n\n# Edit distance functions (Norvig's approach with Damerau-Levenshtein improvements)\ndef edits1(word):\n    \"\"\"Generate all edits that are one edit away from `word`.\"\"\"\n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    deletes = [L + R[1:] for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n    inserts = [L + c + R for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word):\n    \"\"\"Generate all edits that are two edits away from `word`.\"\"\"\n    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))\n\ndef known(words):\n    \"\"\"Return subset of words that appear in the dictionary.\"\"\"\n    return set(w for w in words if w in WORDS)\n\n# Probability based on word frequency\ndef P(word):\n    \"\"\"Probability of word based on frequency.\"\"\"\n    N = sum(WORDS.values())\n    return WORDS[word] / N if word in WORDS else 1e-10\n\n# Bigram probability\ndef P_bigram(prev_word, word):\n    \"\"\"Conditional probability P(word | prev_word) using bigram data.\"\"\"\n    pair = (prev_word, word)\n    if pair in BIGRAMS and prev_word in WORDS:\n        return BIGRAMS[pair] / WORDS[prev_word]\n    return 0  # Fallback to unigram if bigram not found\ndef P_trigram(w1, w2, w3):\n    \"\"\"Conditional probability P(w3 | w1, w2) using trirgram data.\"\"\"\n    tri = (w1, w2, w3)\n    bi = (w1, w2)\n    if tri in TRIGRAMS and bi in BIGRAMS: \n        return TRIGRAMS[tri] / BIGRAMS[bi]\n    return 0\n\ndef P_fourgram(w1, w2, w3, w4):\n    \"\"\"Conditional probability P(w4 | w1, w2, w3) using fourgram data.\"\"\"\n    quad = (w1, w2, w3, w4)\n    tri = (w1, w2, w3)\n    if quad in FOURGRAMS and tri in TRIGRAMS:\n        return FOURGRAMS[quad] / TRIGRAMS[tri]\n    return 0\n# def P_bigram(prev_word, word):\n#     \"\"\"Conditional probability P(word | prev_word) using bigram data.\"\"\"\n#     pair = (prev_word, word)\n#     if pair in FIGRAMS and prev_word in WORDS:\n#         return FIGRAMS[pair] / WORDS[prev_word]\n#     return P(word)  # Fallback to unigram if bigram not found\n\n# Context-sensitive correction\ndef correction1(word, prev_word=None ,fith_prev_word=None):\n    \"\"\"Correct a word with context from the previous word.\"\"\"\n      # No correction needed if word is known\n    \n    # Generate candidates\n    candidates = known([word]) or known(edits1(word)) or known(keyboard_edits(word)) or known(edits2(word)) or {word}\n    if word in WORDS:\n        return max(candidates, key=lambda w: 0.3 * P_bigram(prev_word, w) + 0.7 * P(w))\n    # Score candidates with bigram probability if context is provided\n    if prev_word:\n        # return max(candidates, key=lambda w: P_bigram(prev_word, w))\n        return max(candidates, key=lambda w: 0.7 * P_bigram(prev_word, w) + 0.3 * P(w))\n\n    return max(candidates, key=P)  # Fallback to unigram if no context\ndef correction2(word, prev_words=None):\n    \"\"\"Correct a word with context from up to the last two words.\"\"\"\n    candidates = known([word]) or known(edits1(word)) or known(keyboard_edits(word)) or known(edits2(word)) or {word}\n    \n    if word in WORDS:\n        return max(candidates, key=lambda w: 0.3 * P(w) + 0.7 * best_ngram_prob(prev_words, w))\n    \n    if prev_words:\n        return max(candidates, key=lambda w: 0.7 * best_ngram_prob(prev_words, w) + 0.3 * P(w))\n    \n    return max(candidates, key=P)  # Fallback to unigram if no context\ndef correction2_without_keyboard(word, prev_words=None):\n    \"\"\"Correct a word with context from up to the last two words.\"\"\"\n    candidates = known([word]) or known(edits1(word)) or  known(edits2(word)) or {word}\n    \n    if word in WORDS:\n        return max(candidates, key=lambda w: 0.3 * P(w) + 0.7 * best_ngram_prob(prev_words, w))\n    \n    if prev_words:\n        return max(candidates, key=lambda w: 0.7 * best_ngram_prob(prev_words, w) + 0.3 * P(w))\n    \n    return max(candidates, key=P)  # Fallback to unigram if no context\ndef correction3(word, prev_words=None):\n        \"\"\"Correct a word with context from up to the last two words.\"\"\"\n        candidates = known([word]) or known(edits1(word)) or  known(edits2(word)) or {word}\n        \n        if word in WORDS:\n            return max(candidates, key=lambda w: 0.7 * P(w) + 0.3 * best_ngram_prob(prev_words, w))\n        \n        if prev_words:\n            return max(candidates, key=lambda w: 0.3 * best_ngram_prob(prev_words, w) + 0.7 * P(w))\n        return max(candidates, key=P)\ndef best_ngram_prob(prev_words, word):\n    \"\"\"Choose the highest probability n-gram where the last n words sequence exists in n-grams data.\"\"\"\n    n = len(prev_words)\n    if n >= 3 and tuple(prev_words[-3:] + [word]) in FOURGRAMS:\n        return P_fourgram(*prev_words[-3:], word)\n    if n >= 2 and tuple(prev_words[-2:] + [word]) in TRIGRAMS:\n        return P_trigram(*prev_words[-2:], word)\n    return P_bigram(prev_words[-1], word) if n >= 1 else 0  \n\n\n\n\n# Correct a full line of text\ndef correct_line(line):\n    \"\"\"Correct a line of text word-by-word with context.\"\"\"\n    words = re.findall(r'\\w+', line.lower())\n    corrected = []\n    for i, word in enumerate(words):\n        prev_word = corrected[-1] if i > 0 else None\n        corrected.append(correction1(word, prev_word))\n    return ' '.join(corrected)\n\ndef correct_line2(line):\n    \"\"\"Correct a line of text word-by-word with context from up to the last three words.\"\"\"\n    words = re.findall(r'\\w+', line.lower())\n    corrected = []\n    \n    for i, word in enumerate(words):\n        prev_words = corrected[-3:]  # Get up to the last three corrected words\n        corrected.append(correction2(word, prev_words))  # Pass multiple previous words\n    \n    return ' '.join(corrected)\ndef correct_line_without_keyboard(line):\n    \"\"\"Correct a line of text word-by-word with context from up to the last three words.\"\"\"\n    words = re.findall(r'\\w+', line.lower())\n    corrected = []\n    \n    for i, word in enumerate(words):\n        prev_words = corrected[-3:]  # Get up to the last three corrected words\n        corrected.append(correction2_without_keyboard(word, prev_words))  # Pass multiple previous words\n    \n    return ' '.join(corrected)\ndef correct_line3(line):\n    \"\"\"Correct a line of text word-by-word with context from up to the last three words.\"\"\"\n    words = re.findall(r'\\w+', line.lower())\n    corrected = []\n    \n    for i, word in enumerate(words):\n        prev_words = corrected[-3:]  # Get up to the last three corrected words\n        corrected.append(correction3(word, prev_words))  # Pass multiple previous words\n    \n    return ' '.join(corrected)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T05:08:18.365454Z","iopub.execute_input":"2025-03-05T05:08:18.365760Z","iopub.status.idle":"2025-03-05T05:08:20.116393Z","shell.execute_reply.started":"2025-03-05T05:08:18.365737Z","shell.execute_reply":"2025-03-05T05:08:20.115511Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Justify your decisions\n\nWrite down justificaitons for your implementation choices. For example, these choices could be:\n- Which ngram dataset to use\n- Which weights to assign for edit1, edit2 or absent words probabilities\n- Beam search parameters\n- etc.","metadata":{"id":"oML-5sJwGRLE"}},{"cell_type":"markdown","source":"# Justification of Implementation Choices\n\n## 1. N-gram Dataset Selection\n**Decision:** I used bigram frequencies from a coca ngrams text corpus as the primary context model, with a fallback to unigram from big.txt probabilities when bigram data is unavailable. Additionally, `correction2` and `correction3` support higher N-grams (trigrams and fourgrams) when available.\n\n**Justification:**\n- **Bigrams** provide a practical level of context by capturing the probability of a word given the previous word (e.g., \"doing sport\" vs. \"dying sport\"). This is sufficient to disambiguate many context-sensitive corrections, as seen in the examples \"dking sport\" and \"dking species.\"\n- **Higher N-grams** (trigrams and fourgrams) were included in `correction2` and `correction3` to leverage more context when available, improving accuracy for sequences where longer dependencies matter. However, I prioritized bigrams as the default because they require less data and computational overhead compared to trigrams or fourgrams, making the solution scalable yet still context-aware.\n- The **fallback to unigram probabilities** ensures the model remains robust even when specific bigram data is missing from the corpus, preventing zero-probability issues.\n\n## 2. Candidate Generation\n**Decision:** I generated correction candidates using three methods:\n- Words one edit away (`edits1`).\n- Words two edits away (`edits2`).\n- Words one keyboard-adjacent edit away (`keyboard_edits`), except in `correction2_without_keyboard` and `correction3`.\n\n**Justification:**\n- **Edits1 and Edits2:** These methods, inherited from Norvig’s solution, cover common typos such as deletions, insertions, substitutions, and transpositions. Including `edits2` ensures the model can handle more severe typos (e.g., \"dking\" → \"doing\"), increasing the chances of finding the correct word.\n- **Keyboard-adjacent edits:** Many real-world typos stem from pressing adjacent keys on a QWERTY keyboard (e.g., 'd' instead of 's'). By incorporating `keyboard_edits` in `correction1` and `correction2`, the model prioritizes corrections plausible for such errors, improving accuracy. I omitted this in `correction2_without_keyboard` and `correction3` to explore the trade-off between complexity and performance, hypothesizing that edit-based candidates might suffice with strong N-gram weighting.\n- **Limitation to Two Edits:** I avoided generating candidates beyond two edits to keep the search space manageable, as further edits would exponentially increase computation time with diminishing returns in accuracy for most cases.\n\n## 3. Probability Weighting\n**Decision:** I implemented different weighting schemes for known and unknown words across the four correction functions:\n- `correction1`: 0.3 * P_bigram + 0.7 * P_unigram (known words), 0.7 * P_bigram + 0.3 * P_unigram (unknown words).\n- `correction2`: 0.3 * P_unigram + 0.7 * P_ngram (known words), 0.7 * P_ngram + 0.3 * P_unigram (unknown words).\n- `correction2_without_keyboard`: Same as `correction2`.\n- `correction3`: 0.7 * P_unigram + 0.3 * P_ngram (known words), 0.3 * P_ngram + 0.7 * P_unigram (unknown words).\n\n**Justification:**\n- **Known vs. Unknown Words:** For known words (already in the dictionary), I generally gave higher weight to unigram probability to avoid overcorrecting correct words based solely on context. For unknown words (likely typos), I prioritized N-gram probability to leverage context for selecting the best correction.\n- **Weight Variations:**\n  - In `correction1`, the weights (0.3/0.7 vs. 0.7/0.3) balance context and frequency, reflecting a conservative approach where unigram dominance preserves known words, while bigram dominance guides typo correction.\n  - In `correction2`, I emphasized higher N-grams (0.7) to exploit richer context when available, assuming that trigram/fourgram data could better disambiguate complex cases.\n  - `correction3` reverses this (0.7 unigram for known words), testing a hypothesis that over-reliance on N-grams might overfit to context, especially with sparse higher N-gram data.\n- **Heuristic Choice:** These weights were chosen heuristically as a starting point, based on the intuition that context matters more for errors and frequency matters more for correct words. They could be fine-tuned with evaluation data for optimal performance.\n\n## 4. Four Correction Algorithms\n**Decision:** I implemented four correction functions (`correction1`, `correction2`, `correction2_without_keyboard`, `correction3`) to explore different approaches:\n- `correction1`: Uses bigrams with keyboard edits.\n- `correction2`: Uses up to fourgrams with keyboard edits.\n- `correction2_without_keyboard`: Uses up to fourgrams without keyboard edits.\n- `correction3`: Uses up to fourgrams with adjusted weights, no keyboard edits.\n\n**Justification:**\n- **Exploration of Trade-offs:** Each function tests a different hypothesis:\n  - `correction1` mirrors a baseline improvement over Norvig with bigrams and keyboard awareness.\n  - `correction2` extends this to higher N-grams, hypothesizing that more context improves accuracy.\n  - `correction2_without_keyboard` isolates the effect of keyboard edits, testing if N-gram strength alone suffices.\n  - `correction3` adjusts weights to prioritize unigram probability, exploring robustness against sparse N-gram data.\n- **Flexibility:** Providing multiple algorithms allows comparison and adaptation based on specific use cases or evaluation results, fulfilling the task’s expectation to innovate beyond Norvig’s solution.\n\n## 5. Handling Unknown Words\n**Decision:** For unknown words, I generate candidates from `edits1`, `keyboard_edits` (where applicable), and `edits2`, then select the highest-scoring option based on weighted probabilities.\n\n**Justification:** This approach ensures the model can propose reasonable corrections even for words not in the dictionary, using both edit distance and context to rank candidates. The inclusion of keyboard edits (in `correction1` and `correction2`) enhances realism, while the fallback to unigram probability ensures a default mechanism when context data is lacking.\n\n## 6. Efficiency Considerations\n**Decision:** I used `Counter` for unigram frequencies and `defaultdict` for bigram frequencies, avoiding complex models like neural networks.\n\n**Justification:**\n- Hash-based structures provide O(1) lookup times, making the correction process fast even for long texts. This is critical for practical spelling correction, aligning with the task’s efficiency improvement goal.\n- Simpler frequency-based models were chosen over neural networks due to their lower computational cost and ease of implementation, sufficient for a context-sensitive proof-of-concept.\n\n## 7. No Beam Search\n**Decision:** I opted for a greedy, word-by-word correction approach instead of beam search.\n\n**Justification:** Beam search excels in sequence modeling tasks with long dependencies (e.g., translation), but spelling correction typically relies on local context (e.g., the previous word or two). A greedy approach is faster and sufficient for this task, avoiding unnecessary complexity without significant accuracy gains.\n\n## 8. Context Window\n**Decision:** `correction1` uses a one-word context (bigrams), while `correction2`, `correction2_without_keyboard`, and `correction3` use up to three previous words (fourgrams).\n\n**Justification:**\n- A one-word context balances simplicity and effectiveness for most cases, as seen in \"dking sport\" vs. \"dking species.\"\n- Extending to three words in the other algorithms tests whether additional context improves accuracy, particularly for ambiguous cases, while keeping the model manageable compared to even larger windows.\n\n## 9. Fallback Mechanism\n**Decision:** If N-gram data is unavailable, I fall back to unigram probability.\n\n**Justification:** This prevents the model from failing on unseen word pairs, ensuring robustness and a reasonable correction even with sparse training data.\n\n## 10. Case Insensitivity\n**Decision:** All text is processed in lowercase.\n\n**Justification:** Spelling correction focuses on word correctness, not case. Handling case separately would complicate the model unnecessarily, and most frequency datasets are case-insensitive.\n\n.\n\n## Summary of Key Improvements Over Norvig’s Solution\n- **Context Sensitivity:** Added N-gram probabilities (bigrams and beyond) to consider surrounding words, unlike Norvig’s unigram-only approach.\n- **Keyboard Awareness:** Incorporated QWERTY-adjacent edits to model realistic typos (except in two variants).\n- **Multiple Algorithms:** Implemented four correction functions to test different weighting and context strategies.\n- **Efficiency:** Maintained fast lookups with dictionary-based structures, enhancing scalability.\n\nThese choices collectively aim to create a spelling corrector that is accurate, context-sensitive, and efficient, addressing the task’s requirements while introducing meaningful improvements over the original Norvig solution.","metadata":{"id":"6Xb_twOmVsC6"}},{"cell_type":"markdown","source":"More on probability Weighting: Detailed Analysis\n\n### Overview of the Decision\nThe probability weighting schemes determine how much influence different probabilistic models (unigram, bigram, or higher N-gram probabilities) have on selecting the best correction candidate for a given word. I implemented distinct weighting strategies for **known words** (words already in the dictionary) and **unknown words** (potential typos not in the dictionary) across four correction functions:\n- **`correction1`:** 0.3 * P_bigram + 0.7 * P_unigram (known words), 0.7 * P_bigram + 0.3 * P_unigram (unknown words).\n- **`correction2`:** 0.3 * P_unigram + 0.7 * P_ngram (known words), 0.7 * P_ngram + 0.3 * P_unigram (unknown words).\n- **`correction2_without_keyboard`:** Same as `correction2`.\n- **`correction3`:** 0.7 * P_unigram + 0.3 * P_ngram (known words), 0.3 * P_ngram + 0.7 * P_unigram (unknown words).\n\nThese weights combine the **unigram probability (P_unigram)**—the frequency of a word in isolation—and the **N-gram probability (P_bigram or P_ngram)**—the likelihood of a word given its preceding context (one word for bigrams, up to three for higher N-grams)—to score correction candidates. The choice of weights directly impacts how the model prioritizes frequency versus context, affecting prediction accuracy, robustness, and behavior in different scenarios.\n\n---\n\n### How Weighting Affects Predictions\n\n#### General Mechanism\nFor each word in the input text:\n1. If the word is **known**, the model evaluates whether to keep it as-is or replace it with a correction candidate based on the weighted probability score.\n2. If the word is **unknown**, the model generates correction candidates (via edits or keyboard-adjacent methods) and selects the one with the highest weighted score.\n3. The final prediction is the candidate with the maximum combined probability, where the weights dictate the balance between **context (N-gram)** and **frequency (unigram)**.\n\nThe differing weights across the functions create distinct prediction behaviors, which I’ll analyze below.\n\n---\n\n#### `correction1`: Balancing Context and Frequency\n- **Known Words:** 0.3 * P_bigram + 0.7 * P_unigram\n  - **Effect:** This weighting heavily favors unigram probability (70%), meaning the model prioritizes a word’s overall frequency over its contextual fit. For a known word, the model is conservative—it’s less likely to “correct” it unless the bigram probability strongly suggests a better alternative.\n  - **Prediction Impact:** If the input is “doing sport” (both words known), the high unigram weight keeps “doing” unless “dying sport” has a significantly higher bigram probability (e.g., in a corpus about mortality in sports). This reduces over-correction of valid words but may miss context-driven errors.\n- **Unknown Words:** 0.7 * P_bigram + 0.3 * P_unigram\n  - **Effect:** Here, bigram probability dominates (70%), emphasizing context over frequency. For an unknown word like “dking” in “dking sport,” the model prefers candidates like “doing” if “doing sport” has a high bigram probability, even if “dying” is more frequent overall.\n  - **Prediction Impact:** This makes predictions context-sensitive, correctly resolving “dking sport” to “doing sport” in a sports context, but it risks overfitting to rare bigrams if the corpus is sparse.\n\n**Overall Behavior:** `correction1` strikes a balance—preserving known words unless context strongly disagrees, while aggressively using context to fix typos. It’s effective for short-context corrections but may falter with ambiguous or sparse bigram data.\n\n---\n\n#### `correction2` and `correction2_without_keyboard`: Leveraging Richer Context\n- **Known Words:** 0.3 * P_unigram + 0.7 * P_ngram\n  - **Effect:** Higher N-gram probability (up to fourgrams, 70%) takes precedence, meaning context from up to three previous words heavily influences whether a known word is kept or corrected. Unigram probability (30%) acts as a tiebreaker.\n  - **Prediction Impact:** For “doing sport” in a longer sequence like “he is doing sport,” if “is doing sport” has a strong trigram probability, “doing” stays. However, if the N-gram strongly favors “is dying sport” (unlikely but possible in a niche corpus), it might overcorrect “doing” to “dying.” This risks altering correct words when N-gram data is noisy or overfitted.\n- **Unknown Words:** 0.7 * P_ngram + 0.3 * P_unigram\n  - **Effect:** N-gram probability again dominates (70%), using richer context to rank correction candidates for typos. The low unigram weight (30%) ensures frequency plays a minor role.\n  - **Prediction Impact:** For “dking” in “he is dking sport,” the model evaluates candidates like “doing” and “dying” based on “is doing sport” or “is dying sport.” If trigrams/fourgrams strongly favor “doing,” it predicts correctly. This excels in disambiguating complex cases but depends heavily on the quality and coverage of N-gram data.\n\n**Overall Behavior:** `correction2` (and its variant) prioritizes context, making it powerful for sequences with clear N-gram patterns (e.g., idiomatic phrases). However, sparse or unreliable higher N-gram data could lead to incorrect predictions, especially for known words.\n\n---\n\n#### `correction3`: Frequency-First Approach\n- **Known Words:** 0.7 * P_unigram + 0.3 * P_ngram\n  - **Effect:** Unigram probability dominates (70%), making the model highly conservative for known words. N-gram context (up to fourgrams, 30%) has less sway.\n  - **Prediction Impact:** For “doing sport,” “doing” is retained unless “doing sport” has an unusually low N-gram probability compared to “dying sport.” This minimizes over-correction but may fail to catch context-driven errors (e.g., “dying species” miskept as “doing species”).\n- **Unknown Words:** 0.3 * P_ngram + 0.7 * P_unigram\n  - **Effect:** Unlike the other functions, unigram probability (70%) outweighs N-gram context (30%). For unknown words, frequency drives predictions more than context.\n  - **Prediction Impact:** For “dking” in “dking sport,” if “doing” is more frequent than “dying” in the unigram data, it’s chosen even if “dying sport” fits the context better. This can lead to context-insensitive corrections (e.g., “doing” over “dying” in “dking species”), reducing accuracy in ambiguous cases.\n\n**Overall Behavior:** `correction3` favors frequency, making it robust against sparse N-gram data but less adept at leveraging context. It’s less likely to overcorrect known words but may produce generic, context-ignorant fixes for typos.\n\n---\n\n### Comparative Effects on Predictions\n\n#### Accuracy in Context-Sensitive Cases\n- **`correction1`:** Excels with bigrams (e.g., “dking sport” → “doing sport”), but limited by one-word context.\n- **`correction2`:** Leverages richer context (e.g., “he is dking sport” → “he is doing sport”), improving accuracy when N-gram data is reliable, but risks over-correction.\n- **`correction3`:** Prioritizes frequency, potentially missing context cues (e.g., “dking species” → “doing species” instead of “dying species”).\n\n#### Robustness to Sparse Data\n- **`correction1`:** Fallback to unigram ensures reasonable predictions when bigram data is missing.\n- **`correction2`:** Relies on higher N-grams, so sparse trigram/fourgram data could degrade performance unless unigram fallback compensates.\n- **`correction3`:** Most robust due to unigram dominance, but at the cost of context sensitivity.\n\n#### Over-Correction Risk\n- **`correction1`:** Moderate—conservative for known words, context-driven for unknowns.\n- **`correction2`:** Highest—strong N-gram weighting may alter correct words based on misleading context.\n- **`correction3`:** Lowest—unigram focus preserves known words but may miss needed corrections.\n\n---\n\n### Practical Implications\n- **Use Case Fit:**\n  - **`correction1`:** Best for general-purpose correction with moderate context needs and limited data.\n  - **`correction2`:** Ideal for applications with rich, reliable N-gram data (e.g., domain-specific texts) where context is critical.\n  - **`correction3`:** Suited for noisy or sparse datasets, prioritizing simplicity and frequency over nuanced context.\n- **Tuning Potential:** The heuristic weights (e.g., 0.7/0.3) could be optimized with evaluation data (e.g., precision/recall on a misspelling test set) to better balance context and frequency for specific domains.\n\n---\n\n### Conclusion\nThe weighting schemes in `correction1`, `correction2`, and `correction3` create a spectrum of prediction behaviors:\n- **Context-Driven (`correction2`):** Maximizes accuracy in well-defined contexts but risks overfitting.\n- **Balanced (`correction1`):** Offers a practical middle ground, suitable for most cases.\n- **Frequency-Driven (`correction3`):** Ensures robustness but sacrifices contextual nuance.\n\nThese choices affect predictions by shaping how the model interprets the trade-off between a word’s inherent likelihood (unigram) and its fit within the sentence (N-gram), ultimately determining its effectiveness across diverse inputs and data conditions.","metadata":{}},{"cell_type":"markdown","source":"## Evaluate on a test set\n\nYour task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies.","metadata":{"id":"46rk65S4GRSe"}},{"cell_type":"code","source":"clean_lines = load_clean_lines('/kaggle/input/tatoeba-english-extracted/eng_sentences.tsv', max_lines=1000)  # Load first 10 sentences\nprint(len(clean_lines))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T05:19:41.423178Z","iopub.execute_input":"2025-03-05T05:19:41.423484Z","iopub.status.idle":"2025-03-05T05:19:41.466667Z","shell.execute_reply.started":"2025-03-05T05:19:41.423461Z","shell.execute_reply":"2025-03-05T05:19:41.465813Z"}},"outputs":[{"name":"stdout","text":"1000\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Dataset Description\nThe dataset used for this evaluation is sourced from tatoeba english, a tab-separated values (TSV) file containing English sentences extracted from the Tatoeba project. Tatoeba is a collaborative, open-source collection of sentences and translations, widely used for natural language processing tasks. This specific file includes English sentences, likely with columns for sentence IDs, language codes, and the sentence text itself. For the evaluation, I loaded a subset of up to 100,000 sentences (though limited by `max_lines` in the code), removed punctuation, and converted the text to lowercase to focus on word-level spelling correction. I then introduced synthetic noise (e.g., random edits with a probability of 0.2) to generate a test set of noisy sentences, which were used to compare the performance of my four correction algorithms against Norvig’s baseline solution.\n","metadata":{}},{"cell_type":"markdown","source":"## Evaluation Issue\nA key limitation of this evaluation is the use of random noise to generate the test set, rather than simulating human-like mistakes. The `add_noise` function applies random edits (e.g., insertions, deletions, substitutions) with a probability of 0.2, which does not fully reflect common human typing errors, such as pressing adjacent keys on a keyboard (e.g., 'd' instead of 's'). This explains why the keyboard-aware method (`correction2`) and the no-keyboard method (`correction2_without_keyboard`) perform identically in terms of accuracy (both at 0.900024). The random noise does not preferentially generate keyboard-adjacent errors, reducing the advantage of the keyboard-aware approach. To better evaluate the keyboard edit feature, a test set with human-like typos (e.g., based on QWERTY adjacency) would be more appropriate.","metadata":{}},{"cell_type":"code","source":"import random\ndef add_noise(word, noise_prob=0.2):\n    \"\"\"Add random noise to a word with given probability.\"\"\"\n    if random.random() > noise_prob:\n        return word\n    edits = list(edits1(word))\n    return random.choice(edits) if edits else word\n\ndef generate_test_set(clean_lines, noise_prob=0.35):\n    \"\"\"Generate noisy versions of clean lines.\"\"\"\n    noisy_lines = []\n    for line in clean_lines:\n        noisy_words = [add_noise(word, noise_prob) for word in re.findall(r'\\w+', line.lower())]\n        noisy_lines.append(' '.join(noisy_words))\n    return noisy_lines\n\n# Load clean lines from extracted Tatoeba eng_sentences.tsv\ndef load_clean_lines(filename='/kaggle/input/tatoeba-english-extracted/eng_sentences.tsv', max_lines=1000):\n    \"\"\"Load a subset of English sentences from the uncompressed TSV file and remove punctuation.\"\"\"\n    clean_lines = []\n    with open(filename, 'r', encoding='utf-8') as file:\n        for i, line in enumerate(file):\n            if i >= max_lines:  # Limit to max_lines for testing\n                break\n            parts = line.strip().split('\\t')\n            if len(parts) == 3:  # Ensure valid TSV row\n                sentence = parts[2]  # Third column is the text\n                # Remove all punctuation using regex\n                cleaned_sentence = re.sub(r'[^\\w\\s]', '', sentence)\n                clean_lines.append(cleaned_sentence)\n    return clean_lines\n\n# Load the dataset\nclean_lines = load_clean_lines('/kaggle/input/tatoeba-english-extracted/eng_sentences.tsv', max_lines=1000)  # Load first 10 sentences\nnoisy_lines = generate_test_set(clean_lines, noise_prob=0.2)\n\n# Compare with Norvig's solution\ndef norvig_correct_line(line):\n    words = re.findall(r'\\w+', line.lower())\n    return ' '.join(max(known([w]) or known(edits1(w)) or known(edits2(w)) or {w}, key=P) for w in words)\n\n# Accuracy metric\ndef accuracy(corrected, target):\n    correct_words = sum(1 for c, t in zip(corrected.split(), target.split()) if c.lower() == t.lower())\n    return correct_words / len(target.split())\n\n# Run evaluation\nprint(\"Evaluation Results:\")\ntotal_my_acc_1 = 0\ntotal_my_acc_2 = 0\ntotal_my_acc2_without_keyboard = 0\ntotal_my_acc_3 = 0\ntotal_norvig_acc = 0\n\nfor i, (clean, noisy) in enumerate(zip(clean_lines, noisy_lines)):\n    my_corrected_1 = correct_line(noisy)\n    my_corrected_2 = correct_line2(noisy)\n    my_corrected_without_keyboard = correct_line_without_keyboard(noisy)\n    my_corrected_3 = correct_line3(noisy)\n    norvig_corrected = norvig_correct_line(noisy)\n    \n    my_acc_1 = accuracy(my_corrected_1, clean)\n    my_acc_2 = accuracy(my_corrected_2, clean)\n    my_acc2_without_keyboard = accuracy(my_corrected_without_keyboard, clean)\n    my_acc_3 = accuracy(my_corrected_3, clean)\n    norvig_acc = accuracy(norvig_corrected, clean)\n    \n    # Accumulate accuracies separately\n    total_my_acc_1 += my_acc_1\n    total_my_acc_2 += my_acc_2\n    total_my_acc2_without_keyboard += my_acc2_without_keyboard\n    total_my_acc_3 += my_acc_3\n    total_norvig_acc += norvig_acc\n    \n    if i % 150 == 0:\n        print(f\"Clean: {clean}\")\n        print(f\"Noisy: {noisy}\")\n        print(f\"My Corrected 1: {my_corrected_1} (Acc: {my_acc_1:.2f})\")\n        print(f\"My Corrected 2: {my_corrected_2} (Acc: {my_acc_2:.2f})\")\n        print(f\"My Corrected (No Keyboard): {my_corrected_without_keyboard} (Acc: {my_acc2_without_keyboard:.2f})\")\n        print(f\"My Corrected 3: {my_corrected_3} (Acc: {my_acc_3:.2f})\")\n        print(f\"Norvig Corrected: {norvig_corrected} (Acc: {norvig_acc:.2f})\\n\")\n\n# Calculate and print average accuracies\nnum_lines = len(clean_lines)\nprint(f\"Average My Accuracy 1: {total_my_acc_1 / num_lines:.6f}\")\nprint(f\"Average My Accuracy 2: {total_my_acc_2 / num_lines:.6f}\")\nprint(f\"Average My Accuracy 2 (No Keyboard): {total_my_acc2_without_keyboard / num_lines:.6f}\")\nprint(f\"Average My Accuracy 3: {total_my_acc_3 / num_lines:.6f}\")\nprint(f\"Average Norvig Accuracy: {total_norvig_acc / num_lines:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T05:10:57.809339Z","iopub.execute_input":"2025-03-05T05:10:57.809627Z","iopub.status.idle":"2025-03-05T05:12:58.287120Z","shell.execute_reply.started":"2025-03-05T05:10:57.809606Z","shell.execute_reply":"2025-03-05T05:12:58.286259Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results:\nClean: Lets try something\nNoisy: lets try something\nMy Corrected 1: lets try something (Acc: 1.00)\nMy Corrected 2: lets try something (Acc: 1.00)\nMy Corrected (No Keyboard): lets try something (Acc: 1.00)\nMy Corrected 3: lets try something (Acc: 1.00)\nNorvig Corrected: lets try something (Acc: 1.00)\n\nClean: I dont like you anymore\nNoisy: x doqnt like you anxymore\nMy Corrected 1: x dont like you anymore (Acc: 0.80)\nMy Corrected 2: x dont like you anymore (Acc: 0.80)\nMy Corrected (No Keyboard): x dont like you anymore (Acc: 0.80)\nMy Corrected 3: x dont like you anymore (Acc: 0.80)\nNorvig Corrected: x dont like you anymore (Acc: 0.80)\n\nClean: How do you feel he inquired\nNoisy: how do you feel ve inquiredw\nMy Corrected 1: how do you feel ve inquired (Acc: 0.83)\nMy Corrected 2: how do you feel ve inquired (Acc: 0.83)\nMy Corrected (No Keyboard): how do you feel ve inquired (Acc: 0.83)\nMy Corrected 3: how do you feel ve inquired (Acc: 0.83)\nNorvig Corrected: how do you feel ve inquired (Acc: 0.83)\n\nClean: Do you have friends in Antigua\nNoisy: do you have frienjs in antigua\nMy Corrected 1: do you have friends in antique (Acc: 0.83)\nMy Corrected 2: do you have friends in antique (Acc: 0.83)\nMy Corrected (No Keyboard): do you have friends in antique (Acc: 0.83)\nMy Corrected 3: do you have friends in antique (Acc: 0.83)\nNorvig Corrected: do you have friends in antique (Acc: 0.83)\n\nClean: You met him at the university\nNoisy: you met him at the university\nMy Corrected 1: you met him at the university (Acc: 1.00)\nMy Corrected 2: you met him at the university (Acc: 1.00)\nMy Corrected (No Keyboard): you met him at the university (Acc: 1.00)\nMy Corrected 3: you met him at the university (Acc: 1.00)\nNorvig Corrected: you met him at the university (Acc: 1.00)\n\nClean: What you dont have is better than what you do have\nNoisy: what you dont have is better than what you do have\nMy Corrected 1: what you dont have is better than what you do have (Acc: 1.00)\nMy Corrected 2: what you dont have is better than what you do have (Acc: 1.00)\nMy Corrected (No Keyboard): what you dont have is better than what you do have (Acc: 1.00)\nMy Corrected 3: what you dont have is better than what you do have (Acc: 1.00)\nNorvig Corrected: what you dont have is better than what you do have (Acc: 1.00)\n\nClean: Check that your username and password are written correctly\nNoisy: check that your username and passjword are written correctly\nMy Corrected 1: check that your surname and password are written correctly (Acc: 0.89)\nMy Corrected 2: check that your surname and password are written correctly (Acc: 0.89)\nMy Corrected (No Keyboard): check that your surname and password are written correctly (Acc: 0.89)\nMy Corrected 3: check that your surname and password are written correctly (Acc: 0.89)\nNorvig Corrected: check that your surname and password are written correctly (Acc: 0.89)\n\nAverage My Accuracy 1: 0.910313\nAverage My Accuracy 2: 0.900024\nAverage My Accuracy 2 (No Keyboard): 0.900024\nAverage My Accuracy 3: 0.900676\nAverage Norvig Accuracy: 0.901572\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Visualization of First Test Results (Tatoeba with Random Noise)\n\nBelow is a bar plot comparing the average accuracies of the spelling correction methods on the Tatoeba dataset with synthetic random noise:\n\n<img src=\"tatoeba_accuracy_comparison_small.png\" alt=\"Average Accuracy Comparison (Tatoeba)\" width=\"1200\">\n\n### Interpretation\n- `Correction 1` achieves the highest accuracy (0.910313), outperforming all other methods.\n- `Correction 2` and `Correction 2 (No Keyboard)` tie at the lowest accuracy (0.900024), indicating no benefit from keyboard-aware edits with random noise.\n- `Correction 3` (0.900676) and Norvig (0.901572) are closely matched, with Norvig slightly ahead of the higher N-gram methods.","metadata":{}},{"cell_type":"code","source":"def load_bea4k_dataset(noisy_file='/kaggle/input/bea4k/test.bea4k.noise', clean_file='/kaggle/input/bea4k/test.bea4k', max_lines=100):\n    noisy_lines = []\n    clean_lines = []\n    with open(noisy_file, 'r', encoding='utf-8') as nf, open(clean_file, 'r', encoding='utf-8') as cf:\n        for i, (n_line, c_line) in enumerate(zip(nf, cf)):\n            if i >= max_lines:  # Limit to max_lines\n                break\n            # Remove punctuation and normalize to lowercase\n            clean_text = ' '.join(re.findall(r'\\w+', c_line.lower()))\n            noisy_text = ' '.join(re.findall(r'\\w+', n_line.lower()))\n            clean_lines.append(clean_text)\n            noisy_lines.append(noisy_text)\n    return clean_lines, noisy_lines\nclean_lines, noisy_lines = load_bea4k_dataset('/kaggle/working/test.bea4k.noise', '/kaggle/working/test.bea4k', max_lines=100000000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T05:20:20.719134Z","iopub.execute_input":"2025-03-05T05:20:20.719499Z","iopub.status.idle":"2025-03-05T05:20:20.783310Z","shell.execute_reply.started":"2025-03-05T05:20:20.719472Z","shell.execute_reply":"2025-03-05T05:20:20.782478Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"len(clean_lines),len(noisy_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T05:20:23.143077Z","iopub.execute_input":"2025-03-05T05:20:23.143407Z","iopub.status.idle":"2025-03-05T05:20:23.149063Z","shell.execute_reply.started":"2025-03-05T05:20:23.143380Z","shell.execute_reply":"2025-03-05T05:20:23.148293Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"(4280, 4280)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"num_lines = len(clean_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T05:41:19.393312Z","iopub.execute_input":"2025-03-05T05:41:19.393631Z","iopub.status.idle":"2025-03-05T05:41:19.397316Z","shell.execute_reply.started":"2025-03-05T05:41:19.393606Z","shell.execute_reply":"2025-03-05T05:41:19.396410Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"len(clean_lines)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T05:41:26.503445Z","iopub.execute_input":"2025-03-05T05:41:26.503761Z","iopub.status.idle":"2025-03-05T05:41:26.508776Z","shell.execute_reply.started":"2025-03-05T05:41:26.503726Z","shell.execute_reply":"2025-03-05T05:41:26.507834Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"4280"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"## Second Test Set: BEA-4K Dataset\n\n### Dataset Description\nFor the second evaluation, I used the BEA-4K dataset, downloaded from Google Drive using the following commands:\n- `download_file_from_google_drive('1QxVnFgp0pgEWmS-113SWEjT8tEhXCVF5', 'test.bea4k')` for the clean text file.\n- `download_file_from_google_drive('1pnCU3OUSE0lNN1T6qY4WWhtHZsW3cg1c', 'test.bea4k.noise')` for the noisy (error-introduced) text file.\n\nThe BEA-4K dataset is a subset of the BEA (Building Educational Applications) dataset, commonly used for grammatical error correction (GEC) and spelling correction research. It contains approximately 4,000 sentence pairs, where `test.bea4k` includes the correct (clean) English sentences, and `test.bea4k.noise` contains the same sentences with human-like errors introduced. These errors are designed to mimic real-world typing mistakes, such as misspellings, keyboard adjacency errors (e.g., \"teh\" for \"the\"), and other common typos, rather than purely random edits. The dataset is likely in a plain text format, with one sentence per line, making it straightforward to process for spelling correction tasks.\n\n### Why This Test is Better\nThe BEA-4K dataset offers several advantages over the previous test set (Tatoeba with synthetic random noise), making it a more robust evaluation for my spelling correction algorithms:\n- **Human-Like Errors:** Unlike the random noise approach (e.g., `add_noise` with a 0.2 probability), which applied uniform edit operations (insertions, deletions, substitutions), the BEA-4K noisy data reflects realistic human typing mistakes. This includes keyboard adjacency errors, transposition of letters, and contextually plausible misspellings, aligning better with the intended use case of my keyboard-aware correction methods (`correction1` and `correction2`).\n- **Discrimination of Keyboard Methods:** In the first test, `correction2` (with keyboard edits) and `correction2_without_keyboard` performed identically (0.900024), likely because random noise didn’t favor keyboard-specific errors. The BEA-4K dataset’s human-like typos allow me to better evaluate the effectiveness of my keyboard-adjacent edit feature, as it should now outperform the no-keyboard variant in correcting errors like \"hte\" to \"the\".\n- **Real-World Relevance:** The BEA-4K dataset is curated for evaluating error correction systems, ensuring that the errors are representative of those encountered in real text (e.g., student writing or casual typing). This contrasts with the synthetic noise in the Tatoeba test, which lacked linguistic or typographical realism.\n- **Standardized Benchmark:** BEA-4K is a recognized dataset in NLP research, providing a standardized benchmark to compare my algorithms against Norvig’s solution and potentially other state-of-the-art systems. This enhances the credibility and comparability of the results.\n\nBy switching to BEA-4K, this second test better assesses the context-sensitivity and keyboard-awareness of my algorithms, addressing the limitations of the first evaluation and offering a more meaningful measure of performance in practical scenarios.","metadata":{}},{"cell_type":"code","source":"import random\nrandom.seed(42)\nimport json\ndef add_noise(word, noise_prob=0.2):\n    \"\"\"Add random noise to a word with given probability.\"\"\"\n    if random.random() > noise_prob:\n        return word\n    edits = list(edits1(word))\n    return random.choice(edits) if edits else word\n\ndef generate_test_set(clean_lines, noise_prob=0.35):\n    \"\"\"Generate noisy versions of clean lines.\"\"\"\n    noisy_lines = []\n    for line in clean_lines:\n        noisy_words = [add_noise(word, noise_prob) for word in re.findall(r'\\w+', line.lower())]\n        noisy_lines.append(' '.join(noisy_words))\n    return noisy_lines\n\n# Load clean lines from extracted Tatoeba eng_sentences.tsv\ndef load_clean_lines(filename='/kaggle/input/tatoeba-english-extracted/eng_sentences.tsv', max_lines=10):\n    \"\"\"Load a subset of English sentences from the uncompressed TSV file.\"\"\"\n    clean_lines = []\n    with open(filename, 'r', encoding='utf-8') as file:\n        for i, line in enumerate(file):\n            if i >= max_lines:  # Limit to max_lines for testing\n                break\n            parts = line.strip().split('\\t')\n            if len(parts) == 3:  # Ensure valid TSV row\n                sentence = parts[2]  # Third column is the text\n                clean_lines.append(sentence)\n    return clean_lines\n\n# Load the dataset\n# clean_lines = load_clean_lines('/kaggle/input/tatoeba-english-extracted/eng_sentences.tsv', max_lines=10)  # Load first 10 sentences\n# noisy_lines = generate_test_set(clean_lines, noise_prob=0.2)\n\n# Compare with Norvig's solution\n\ndef norvig_correct_line(line):\n    words = re.findall(r'\\w+', line.lower())\n    return ' '.join(max(known([w]) or known(edits1(w)) or known(edits2(w)) or {w}, key=P) for w in words)\n\n# Accuracy metric\ndef accuracy(corrected, target):\n    correct_words = sum(1 for c, t in zip(corrected.split(), target.split()) if c.lower() == t.lower())\n    return correct_words / len(target.split())\n\n# Run evaluation\nprint(\"Evaluation Results:\")\ntotal_my_acc_1 = 0\ntotal_my_acc_2 = 0\ntotal_my_acc2_without_keyboard = 0\ntotal_my_acc_3 = 0\ntotal_norvig_acc = 0\nfor i, (clean, noisy) in enumerate(zip(clean_lines, noisy_lines)):\n    my_corrected_1 = correct_line(noisy)\n    my_corrected_2 = correct_line2(noisy)\n    my_corrected_without_keyboard = correct_line_without_keyboard(noisy)\n    my_corrected_3 = correct_line3(noisy)\n    norvig_corrected = norvig_correct_line(noisy)\n    \n    my_acc_1 = accuracy(my_corrected_1, clean)\n    my_acc_2 = accuracy(my_corrected_2, clean)\n    my_acc2_without_keyboard = accuracy(my_corrected_without_keyboard, clean)\n    my_acc_3 = accuracy(my_corrected_3, clean)\n    norvig_acc = accuracy(norvig_corrected, clean)\n    \n    # Accumulate accuracies separately\n    total_my_acc_1 += my_acc_1\n    total_my_acc_2 += my_acc_2\n    total_my_acc2_without_keyboard += my_acc2_without_keyboard\n    total_my_acc_3 += my_acc_3\n    total_norvig_acc += norvig_acc\n    \n    if i % 150 == 0:\n        print(f\"Clean: {clean}\")\n        print(f\"Noisy: {noisy}\")\n        print(f\"My Corrected 1: {my_corrected_1} (Acc: {my_acc_1:.2f})\")\n        print(f\"My Corrected 2: {my_corrected_2} (Acc: {my_acc_2:.2f})\")\n        print(f\"My Corrected (No Keyboard): {my_corrected_without_keyboard} (Acc: {my_acc2_without_keyboard:.2f})\")\n        print(f\"My Corrected 3: {my_corrected_3} (Acc: {my_acc_3:.2f})\")\n        print(f\"Norvig Corrected: {norvig_corrected} (Acc: {norvig_acc:.2f})\\n\")\n\n# Calculate and print average accuracies\nnum_lines = len(clean_lines)\nprint(f\"Average My Accuracy 1: {total_my_acc_1 / num_lines:.6f}\")\nprint(f\"Average My Accuracy 2: {total_my_acc_2 / num_lines:.6f}\")\nprint(f\"Average My Accuracy 2 (No Keyboard): {total_my_acc2_without_keyboard / num_lines:.6f}\")\nprint(f\"Average My Accuracy 3: {total_my_acc_3 / num_lines:.6f}\")\nprint(f\"Average Norvig Accuracy: {total_norvig_acc / num_lines:.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T05:42:32.176071Z","iopub.execute_input":"2025-03-05T05:42:32.176431Z","iopub.status.idle":"2025-03-05T06:00:02.927859Z","shell.execute_reply.started":"2025-03-05T05:42:32.176402Z","shell.execute_reply":"2025-03-05T06:00:02.926799Z"}},"outputs":[{"name":"stdout","text":"Evaluation Results:\nClean: i have just received the letter which lets me know that i have won the first prize\nNoisy: i have just recieved the letter which lets me know that i have won the first prize\nMy Corrected 1: i have just received the letter which lets me know that i have won the first prize (Acc: 1.00)\nMy Corrected 2: i have just received the letter which lets me know that i have won the first prize (Acc: 1.00)\nMy Corrected (No Keyboard): i have just received the letter which lets me know that i have won the first prize (Acc: 1.00)\nMy Corrected 3: i have just received the letter which lets me know that i have won the first prize (Acc: 1.00)\nNorvig Corrected: i have just received the letter which lets me know that i have won the first prize (Acc: 1.00)\n\nClean: yours sincerely\nNoisy: yours sincerly\nMy Corrected 1: yours sincerely (Acc: 1.00)\nMy Corrected 2: yours sincerely (Acc: 1.00)\nMy Corrected (No Keyboard): yours sincerely (Acc: 1.00)\nMy Corrected 3: yours sincerely (Acc: 1.00)\nNorvig Corrected: yours sincerely (Acc: 1.00)\n\nClean: it s like walking in the clouds you feel like you are flying\nNoisy: it s like walking in the clouds you feel like you are flaing\nMy Corrected 1: it s like walking in the clouds you feel like you are flying (Acc: 1.00)\nMy Corrected 2: it s like walking in the clouds you feel like you are flying (Acc: 1.00)\nMy Corrected (No Keyboard): it s like walking in the clouds you feel like you are flying (Acc: 1.00)\nMy Corrected 3: it s like walking in the clouds you feel like you are flying (Acc: 1.00)\nNorvig Corrected: it s like walking in the clouds you feel like you are flying (Acc: 1.00)\n\nClean: in my opinion the easiest way to get there from the conference would be by taking the picadelly line\nNoisy: in my oppinion the easiest way to get there from the conference would be by taking the picadelly line\nMy Corrected 1: in my opinion the easiest way to get there from the conference would be by taking the picadelly line (Acc: 1.00)\nMy Corrected 2: in my opinion the easiest way to get there from the conference would be by taking the picadelly line (Acc: 1.00)\nMy Corrected (No Keyboard): in my opinion the easiest way to get there from the conference would be by taking the picadelly line (Acc: 1.00)\nMy Corrected 3: in my opinion the easiest way to get there from the conference would be by taking the picadelly line (Acc: 1.00)\nNorvig Corrected: in my opinion the easiest way to get there from the conference would be by taking the picadelly line (Acc: 1.00)\n\nClean: it is a modern hotel near our college and it is also very convenient for the airport you can take either bus or taxies\nNoisy: it is a modern hotel near our college and it is also very convenent for the airport you can take either bus or taxies\nMy Corrected 1: it is a modern hotel near our college and it is also very convenient for the airport you can take either bus or taxes (Acc: 0.96)\nMy Corrected 2: it is a modern hotel near our college and it is also very convenient for the airport you can take either bus or taxes (Acc: 0.96)\nMy Corrected (No Keyboard): it is a modern hotel near our college and it is also very convenient for the airport you can take either bus or taxes (Acc: 0.96)\nMy Corrected 3: it is a modern hotel near our college and it is also very convenient for the airport you can take either bus or taxes (Acc: 0.96)\nNorvig Corrected: it is a modern hotel near our college and it is also very convenient for the airport you can take either bus or taxes (Acc: 0.96)\n\nClean: the aim of this report is to suggest which activities in our daily life at school should be filmed to give the other students an idea of what we usually do not only during the lessons but also the rest of the time\nNoisy: the aim of this report is to suggest which activities in our daily life at school shoud be filmed to give the other students an idea of what we usually do not only during the lessons but also the rest of the time\nMy Corrected 1: the aim of this report is to suggest which activities in our daily life at school should be filled to give the other students an idea of what we usually do not only during the lessons but also the rest of the time (Acc: 0.98)\nMy Corrected 2: the aim of this report is to suggest which activities in our daily life at school should be filled to give the other students an idea of what we usually do not only during the lessons but also the rest of the time (Acc: 0.98)\nMy Corrected (No Keyboard): the aim of this report is to suggest which activities in our daily life at school should be filled to give the other students an idea of what we usually do not only during the lessons but also the rest of the time (Acc: 0.98)\nMy Corrected 3: the aim of this report is to suggest which activities in our daily life at school should be filled to give the other students an idea of what we usually do not only during the lessons but also the rest of the time (Acc: 0.98)\nNorvig Corrected: the aim of this report is to suggest which activities in our daily life at school should be filled to give the other students an idea of what we usually do not only during the lessons but also the rest of the time (Acc: 0.98)\n\nClean: for the activities i chose climbing because i took a course for 2 weeks last year and now i have a good level of proficiency for the other one i chose photography but i am not a professional i just take some pictures on my holiday\nNoisy: for the activities i chose climbing because i took a cours for 2 weeks last year and now i have a good level of proficiency for the other one i chose photography but i am not a proffesional i just take some pictures on my holiday\nMy Corrected 1: for the activities i chose climbing because i took a court for 2 weeks last year and now i have a good level of proficient for the other one i chose photography but i am not a professional i just take some pictures on my holiday (Acc: 0.96)\nMy Corrected 2: for the activities i chose climbing because i took a court for 2 weeks last year and now i have a good level of proficient for the other one i chose photography but i am not a professional i just take some pictures on my holiday (Acc: 0.96)\nMy Corrected (No Keyboard): for the activities i chose climbing because i took a court for 2 weeks last year and now i have a good level of proficient for the other one i chose photography but i am not a professional i just take some pictures on my holiday (Acc: 0.96)\nMy Corrected 3: for the activities i chose climbing because i took a court for 2 weeks last year and now i have a good level of proficient for the other one i chose photography but i am not a professional i just take some pictures on my holiday (Acc: 0.96)\nNorvig Corrected: for the activities i chose climbing because i took a course for 2 weeks last year and now i have a good level of proficient for the other one i chose photography but i am not a professional i just take some pictures on my holiday (Acc: 0.98)\n\nClean: we should n t disregard and neglect to film the buildings and the gardens of our beautiful college especially if we take into consideration the fact that it is in the city of shakespeare\nNoisy: we should n t disregard and neglect to film the buildings and the gardens of our beautiful college especially if we take into consideration the fact that it is in the city of shakespear\nMy Corrected 1: we should n t disregard and neglect to film the buildings and the gardens of our beautiful college especially if we take into consideration the fact that it is in the city of shakespeare (Acc: 1.00)\nMy Corrected 2: we should n t disregard and neglect to film the buildings and the gardens of our beautiful college especially if we take into consideration the fact that it is in the city of shakespeare (Acc: 1.00)\nMy Corrected (No Keyboard): we should n t disregard and neglect to film the buildings and the gardens of our beautiful college especially if we take into consideration the fact that it is in the city of shakespeare (Acc: 1.00)\nMy Corrected 3: we should n t disregard and neglect to film the buildings and the gardens of our beautiful college especially if we take into consideration the fact that it is in the city of shakespeare (Acc: 1.00)\nNorvig Corrected: we should n t disregard and neglect to film the buildings and the gardens of our beautiful college especially if we take into consideration the fact that it is in the city of shakespeare (Acc: 1.00)\n\nClean: moreover men prefer to do sports because shopping wastes a lot of time which is contrary to the tastes of women because if they have any free time they will go shopping\nNoisy: moreover men prefer to do sports because shopping wastes a lot of time which is controry to the tastes of women because if they have any free time they will go shopping\nMy Corrected 1: moreover men prefer to do sports because shopping wastes a lot of time which is contrary to the tastes of women because if they have any free time they will go shopping (Acc: 1.00)\nMy Corrected 2: moreover men prefer to do sports because shopping wastes a lot of time which is contrary to the tastes of women because if they have any free time they will go shopping (Acc: 1.00)\nMy Corrected (No Keyboard): moreover men prefer to do sports because shopping wastes a lot of time which is contrary to the tastes of women because if they have any free time they will go shopping (Acc: 1.00)\nMy Corrected 3: moreover men prefer to do sports because shopping wastes a lot of time which is contrary to the tastes of women because if they have any free time they will go shopping (Acc: 1.00)\nNorvig Corrected: moreover men prefer to do sports because shopping wastes a lot of time which is contrary to the tastes of women because if they have any free time they will go shopping (Acc: 1.00)\n\nClean: i support this idea which is convenient both for the public and the organisers\nNoisy: i support this idea which is convinient both for the public and the organisators\nMy Corrected 1: i support this idea which is convenient both for the public and the organisators (Acc: 0.93)\nMy Corrected 2: i support this idea which is convenient both for the public and the organisators (Acc: 0.93)\nMy Corrected (No Keyboard): i support this idea which is convenient both for the public and the organisators (Acc: 0.93)\nMy Corrected 3: i support this idea which is convenient both for the public and the organisators (Acc: 0.93)\nNorvig Corrected: i support this idea which is convenient both for the public and the organisators (Acc: 0.93)\n\nClean: i am therefore asking for compensation for our disappointing evening and hope we can reach a solution as soon as possible\nNoisy: i am therefore asking for compensation for our disappointing evening and hope we can reach a sollution as soon as possible\nMy Corrected 1: i am therefore asking for compensation for our disappointing evening and hope we can reach a solution as soon as possible (Acc: 1.00)\nMy Corrected 2: i am therefore asking for compensation for our disappointing evening and hope we can reach a solution as soon as possible (Acc: 1.00)\nMy Corrected (No Keyboard): i am therefore asking for compensation for our disappointing evening and hope we can reach a solution as soon as possible (Acc: 1.00)\nMy Corrected 3: i am therefore asking for compensation for our disappointing evening and hope we can reach a solution as soon as possible (Acc: 1.00)\nNorvig Corrected: i am therefore asking for compensation for our disappointing evening and hope we can reach a solution as soon as possible (Acc: 1.00)\n\nClean: firstly log cabins are more comfortable than tents\nNoisy: firstly log cabins are more comportable than tents\nMy Corrected 1: firstly log cabins are more comfortable than tents (Acc: 1.00)\nMy Corrected 2: firstly log cabins are more comfortable than tents (Acc: 1.00)\nMy Corrected (No Keyboard): firstly log cabins are more comfortable than tents (Acc: 1.00)\nMy Corrected 3: firstly log cabins are more comfortable than tents (Acc: 1.00)\nNorvig Corrected: firstly log cabins are more comfortable than tents (Acc: 1.00)\n\nClean: they are much more comfortable than tents\nNoisy: they are much more confortable than tents\nMy Corrected 1: they are much more comfortable than tents (Acc: 1.00)\nMy Corrected 2: they are much more comfortable than tents (Acc: 1.00)\nMy Corrected (No Keyboard): they are much more comfortable than tents (Acc: 1.00)\nMy Corrected 3: they are much more comfortable than tents (Acc: 1.00)\nNorvig Corrected: they are much more comfortable than tents (Acc: 1.00)\n\nClean: you can imagine how excited i was especially because i always wanted some photographs of them and this was a special moment and i realised i had n t brought my camera i was very sad\nNoisy: you can immagine how excited i was especially because i always wanted some photographs of them and this was a special moment and i realice i had n t brought my camera i was very sad\nMy Corrected 1: you can imagine how excited i was especially because i always wanted some photographs of them and this was a special moment and i realize i had n t brought my camera i was very sad (Acc: 0.97)\nMy Corrected 2: you can imagine how excited i was especially because i always wanted some photographs of them and this was a special moment and i realize i had n t brought my camera i was very sad (Acc: 0.97)\nMy Corrected (No Keyboard): you can imagine how excited i was especially because i always wanted some photographs of them and this was a special moment and i realize i had n t brought my camera i was very sad (Acc: 0.97)\nMy Corrected 3: you can imagine how excited i was especially because i always wanted some photographs of them and this was a special moment and i realize i had n t brought my camera i was very sad (Acc: 0.97)\nNorvig Corrected: you can imagine how excited i was especially because i always wanted some photographs of them and this was a special moment and i realize i had n t brought my camera i was very sad (Acc: 0.97)\n\nClean: and please tell me how much money i am supposed to need excluding transport and accommodation\nNoisy: and please tell me how much money i am supposed to need excluding transport and accomodation\nMy Corrected 1: and please tell me how much money i am supposed to need excluding transport and accommodation (Acc: 1.00)\nMy Corrected 2: and please tell me how much money i am supposed to need excluding transport and accommodation (Acc: 1.00)\nMy Corrected (No Keyboard): and please tell me how much money i am supposed to need excluding transport and accommodation (Acc: 1.00)\nMy Corrected 3: and please tell me how much money i am supposed to need excluding transport and accommodation (Acc: 1.00)\nNorvig Corrected: and please tell me how much money i am supposed to need excluding transport and accommodation (Acc: 1.00)\n\nClean: it was difficult for me because i am afraid of water\nNoisy: it was diffucult for me because i am afraid of watter\nMy Corrected 1: it was difficult for me because i am afraid of water (Acc: 1.00)\nMy Corrected 2: it was difficult for me because i am afraid of water (Acc: 1.00)\nMy Corrected (No Keyboard): it was difficult for me because i am afraid of water (Acc: 1.00)\nMy Corrected 3: it was difficult for me because i am afraid of water (Acc: 1.00)\nNorvig Corrected: it was difficult for me because i am afraid of matter (Acc: 0.91)\n\nClean: when we think about clothes in the future we should think about the environment first\nNoisy: when we think about clothes in the future we should think about the enviroment first\nMy Corrected 1: when we think about clothes in the future we should think about the environment first (Acc: 1.00)\nMy Corrected 2: when we think about clothes in the future we should think about the environment first (Acc: 1.00)\nMy Corrected (No Keyboard): when we think about clothes in the future we should think about the environment first (Acc: 1.00)\nMy Corrected 3: when we think about clothes in the future we should think about the environment first (Acc: 1.00)\nNorvig Corrected: when we think about clothes in the future we should think about the environment first (Acc: 1.00)\n\nClean: also i wanted to tell you that the show was very boring\nNoisy: alslo i wanted to tell you that the show was very borring\nMy Corrected 1: also i wanted to tell you that the show was very boring (Acc: 1.00)\nMy Corrected 2: also i wanted to tell you that the show was very boring (Acc: 1.00)\nMy Corrected (No Keyboard): also i wanted to tell you that the show was very boring (Acc: 1.00)\nMy Corrected 3: also i wanted to tell you that the show was very boring (Acc: 1.00)\nNorvig Corrected: also i wanted to tell you that the show was very barring (Acc: 0.92)\n\nClean: i would prefer to stay in a tent to staying in a cabin because it is something i feel more familiar with i used to go camping with my family every summer\nNoisy: i would prefer to stay in a tent to staying in a cabin because it is something i feel more familia with i used to go camping with my family every summer\nMy Corrected 1: i would prefer to stay in a tent to staying in a cabin because it is something i feel more familiar with i used to go camping with my family every summer (Acc: 1.00)\nMy Corrected 2: i would prefer to stay in a tent to staying in a cabin because it is something i feel more familiar with i used to go camping with my family every summer (Acc: 1.00)\nMy Corrected (No Keyboard): i would prefer to stay in a tent to staying in a cabin because it is something i feel more familiar with i used to go camping with my family every summer (Acc: 1.00)\nMy Corrected 3: i would prefer to stay in a tent to staying in a cabin because it is something i feel more familiar with i used to go camping with my family every summer (Acc: 1.00)\nNorvig Corrected: i would prefer to stay in a tent to staying in a cabin because it is something i feel more familiar with i used to go camping with my family every summer (Acc: 1.00)\n\nClean: i had been looking forward to seeing my favourite actor danny brook but he had been replaced by some other actor\nNoisy: i had been looking forward to seeing my favorite actor danny brook but he had been replaced by some other actor\nMy Corrected 1: i had been looking forward to seeing my favorite actor dandy brook but he had been replaced by some other actor (Acc: 0.90)\nMy Corrected 2: i had been looking forward to seeing my favorite actor dandy brook but he had been replaced by some other actor (Acc: 0.90)\nMy Corrected (No Keyboard): i had been looking forward to seeing my favorite actor dandy brook but he had been replaced by some other actor (Acc: 0.90)\nMy Corrected 3: i had been looking forward to seeing my favorite actor dandy brook but he had been replaced by some other actor (Acc: 0.90)\nNorvig Corrected: i had been looking forward to seeing my favorite actor dandy brook but he had been replaced by some other actor (Acc: 0.90)\n\nClean: after the show had finished i went to the theatre restaurant to have something to drink and relax but it was closed because the staff was on holiday\nNoisy: after the show had finished i went to the theatre restaurand to hav something to drink and relax but it was closed because the staff was on holiday\nMy Corrected 1: after the show had finished i went to the theatre restaurant to have something to drink and relax but it was closed because the staff was on holiday (Acc: 1.00)\nMy Corrected 2: after the show had finished i went to the theatre restaurant to have something to drink and relax but it was closed because the staff was on holiday (Acc: 1.00)\nMy Corrected (No Keyboard): after the show had finished i went to the theatre restaurant to have something to drink and relax but it was closed because the staff was on holiday (Acc: 1.00)\nMy Corrected 3: after the show had finished i went to the theatre restaurant to have something to drink and relax but it was closed because the staff was on holiday (Acc: 1.00)\nNorvig Corrected: after the show had finished i went to the theatre restaurant to had something to drink and relax but it was closed because the staff was on holiday (Acc: 0.96)\n\nClean: in the area of publicity we were about 50 guys all of us investigating calling radio stations organizing the reporters specifying where the press would be for the interview that took place afterwards yet it was incredibly satisfying when everything turned out just fine\nNoisy: in the area of publicity we were about 50 guys all of us investigating calling radio stations organizing the reporters specifying where the press would be for the interview that took place afterwards yet it was incredibly satisfation when everything turned out just fine\nMy Corrected 1: in the area of publicity we were about 50 guns all of us investigation calling radio stations organizing the reporter specifying where the press would be for the interview that took place afterwards yet it was incredibly satisfaction when everything turned out just fine (Acc: 0.91)\nMy Corrected 2: in the area of publicity we were about 50 guns all of us investigation calling radio stations organizing the reporter specifying where the press would be for the interview that took place afterwards yet it was incredibly satisfaction when everything turned out just fine (Acc: 0.91)\nMy Corrected (No Keyboard): in the area of publicity we were about 50 guns all of us investigation calling radio stations organizing the reporter specifying where the press would be for the interview that took place afterwards yet it was incredibly satisfaction when everything turned out just fine (Acc: 0.91)\nMy Corrected 3: in the area of publicity we were about 50 guns all of us investigation calling radio stations organizing the reporter specifying where the press would be for the interview that took place afterwards yet it was incredibly satisfaction when everything turned out just fine (Acc: 0.91)\nNorvig Corrected: in the area of publicity we were about 50 guns all of us investigation calling radio stations organizing the reporter specifying where the press would be for the interview that took place afterwards yet it was incredibly satisfaction when everything turned out just fine (Acc: 0.91)\n\nClean: my main work was to take people to their seats because of a concert hall which was the millennium dome\nNoisy: my main work was to take people to their seats because of a concert hall which was the millenium dome\nMy Corrected 1: my main work was to take people to their seats because of a concert hall which was the millennium dome (Acc: 1.00)\nMy Corrected 2: my main work was to take people to their seats because of a concert hall which was the millennium dome (Acc: 1.00)\nMy Corrected (No Keyboard): my main work was to take people to their seats because of a concert hall which was the millennium dome (Acc: 1.00)\nMy Corrected 3: my main work was to take people to their seats because of a concert hall which was the millennium dome (Acc: 1.00)\nNorvig Corrected: my main work was to take people to their seats because of a concert hall which was the millennium dome (Acc: 1.00)\n\nClean: i had always dreamt of being near the cranberies but i had never thought that the dream could become reality\nNoisy: i had always dreamt of being near the cranberies but i had never thought that the dream could become reallity\nMy Corrected 1: i had always dreamt of being near the canneries but i had never thought that the dream could become reality (Acc: 0.95)\nMy Corrected 2: i had always dreamt of being near the canneries but i had never thought that the dream could become reality (Acc: 0.95)\nMy Corrected (No Keyboard): i had always dreamt of being near the canneries but i had never thought that the dream could become reality (Acc: 0.95)\nMy Corrected 3: i had always dreamt of being near the canneries but i had never thought that the dream could become reality (Acc: 0.95)\nNorvig Corrected: i had always dreamt of being near the canneries but i had never thought that the dream could become reality (Acc: 0.95)\n\nClean: as i was so looking forward to seeing danny s performance i was very disappointed\nNoisy: as i was so looking foreward to seeing danny s performance i was very disappointed\nMy Corrected 1: as i was so looking forward to seeing dandy s performance i was very disappointed (Acc: 0.93)\nMy Corrected 2: as i was so looking forward to seeing dandy s performance i was very disappointed (Acc: 0.93)\nMy Corrected (No Keyboard): as i was so looking forward to seeing dandy s performance i was very disappointed (Acc: 0.93)\nMy Corrected 3: as i was so looking forward to seeing dandy s performance i was very disappointed (Acc: 0.93)\nNorvig Corrected: as i was so looking forward to seeing dandy s performance i was very disappointed (Acc: 0.93)\n\nClean: it s a great opportunity for me to participate in your camp california because normally i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children\nNoisy: it s a great opportunity for me to participate in your camp california because normaly i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children\nMy Corrected 1: it s a great opportunity for me to participate in your camp california because normally i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children (Acc: 1.00)\nMy Corrected 2: it s a great opportunity for me to participate in your camp california because normally i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children (Acc: 1.00)\nMy Corrected (No Keyboard): it s a great opportunity for me to participate in your camp california because normally i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children (Acc: 1.00)\nMy Corrected 3: it s a great opportunity for me to participate in your camp california because normally i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children (Acc: 1.00)\nNorvig Corrected: it s a great opportunity for me to participate in your camp california because normal i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children (Acc: 0.98)\n\nClean: first i think we should rent bigger halls so that we can make a better sound and give more space for the audience\nNoisy: first i think we shoul rent bigger halls so that we can make a better sound and give more space for the audience\nMy Corrected 1: first i think we should rent bigger halls so that we can make a better sound and give more space for the audience (Acc: 1.00)\nMy Corrected 2: first i think we shout rent bigger halls so that we can make a better sound and give more space for the audience (Acc: 0.96)\nMy Corrected (No Keyboard): first i think we shout rent bigger halls so that we can make a better sound and give more space for the audience (Acc: 0.96)\nMy Corrected 3: first i think we shout rent bigger halls so that we can make a better sound and give more space for the audience (Acc: 0.96)\nNorvig Corrected: first i think we should rent bigger halls so that we can make a better sound and give more space for the audience (Acc: 1.00)\n\nClean: that makes those people frustrated and they do n t enjoy themselves\nNoisy: that makes those people frustrated and they do n t enjoy theirselves\nMy Corrected 1: that makes those people frustrated and they do n t enjoy themselves (Acc: 1.00)\nMy Corrected 2: that makes those people frustrated and they do n t enjoy themselves (Acc: 1.00)\nMy Corrected (No Keyboard): that makes those people frustrated and they do n t enjoy themselves (Acc: 1.00)\nMy Corrected 3: that makes those people frustrated and they do n t enjoy themselves (Acc: 1.00)\nNorvig Corrected: that makes those people frustrated and they do n t enjoy themselves (Acc: 1.00)\n\nClean: i have given a questionnaire to other students in my class to know their preferences regarding this choice and we all believe that the first lesson that should be filmed is philosophy\nNoisy: i have given a questionnaire to other students in my class to know their preferences regarding this choise and we all believe that the first lesson that should be filmed is philosophy\nMy Corrected 1: i have given a questionnaire to other students in my class to know their preference regarding this choice and we all believe that the first lesson that should be filled is philosophy (Acc: 0.94)\nMy Corrected 2: i have given a questionnaire to other students in my class to know their preference regarding this choice and we all believe that the first lesson that should be filled is philosophy (Acc: 0.94)\nMy Corrected (No Keyboard): i have given a questionnaire to other students in my class to know their preference regarding this choice and we all believe that the first lesson that should be filled is philosophy (Acc: 0.94)\nMy Corrected 3: i have given a questionnaire to other students in my class to know their preference regarding this choice and we all believe that the first lesson that should be filled is philosophy (Acc: 0.94)\nNorvig Corrected: i have given a questionnaire to other students in my class to know their references regarding this choose and we all believe that the first lesson that should be filled is philosophy (Acc: 0.91)\n\nAverage My Accuracy 1: 0.964859\nAverage My Accuracy 2: 0.962504\nAverage My Accuracy 2 (No Keyboard): 0.962504\nAverage My Accuracy 3: 0.962402\nAverage Norvig Accuracy: 0.960724\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"## Visualization of Results\n\nBelow is a bar plot comparing the average accuracies of the spelling correction methods on the BEA-4K dataset:\n\n<img src=\"accuracy_comparison.png\" alt=\"Average Accuracy Comparison\" width=\"1400\" >\n\n### Interpretation\n- `Correction 1` achieves the highest accuracy (0.964859), followed closely by `Correction 2` and `Correction 2 (No Keyboard)` (both 0.962504).\n- Norvig’s method has the lowest accuracy (0.960724), highlighting the benefit of context-aware corrections.","metadata":{}},{"cell_type":"markdown","source":"## Analysis of Second Test Set Results (BEA-4K Dataset)\n\n### Benchmark Results\nThe average accuracies for the correction algorithms, evaluated on the BEA-4K dataset (`test.bea4k` clean and `test.bea4k.noise`), are as follows:\n\n- **Average My Accuracy 1:** 0.964859\n- **Average My Accuracy 2:** 0.962504\n- **Average My Accuracy 2 (No Keyboard):** 0.962504\n- **Average My Accuracy 3:** 0.962402\n- **Average Norvig Accuracy:** 0.960724\n\n### Sample Outputs\nBelow are three example sentence pairs from the evaluation, showing the clean text, noisy input, and the outputs of each correction method along with their per-sentence accuracies:\n\n#### Example 1\n- **Clean:** `i have given a questionnaire to other students in my class to know their preferences regarding this choice and we all believe that the first lesson that should be filmed is philosophy`\n- **Noisy:** `i have given a questionnaire to other students in my class to know their preferences regarding this choise and we all believe that the first lesson that should be filmed is philosophy`\n- **My Corrected 1:** `i have given a questionnaire to other students in my class to know their preference regarding this choice and we all believe that the first lesson that should be filled is philosophy` (Acc: 0.94)\n- **My Corrected 2:** `i have given a questionnaire to other students in my class to know their preference regarding this choice and we all believe that the first lesson that should be filled is philosophy` (Acc: 0.94)\n- **My Corrected (No Keyboard):** `i have given a questionnaire to other students in my class to know their preference regarding this choice and we all believe that the first lesson that should be filled is philosophy` (Acc: 0.94)\n- **My Corrected 3:** `i have given a questionnaire to other students in my class to know their preference regarding this choice and we all believe that the first lesson that should be filled is philosophy` (Acc: 0.94)\n- **Norvig Corrected:** `i have given a questionnaire to other students in my class to know their references regarding this choose and we all believe that the first lesson that should be filled is philosophy` (Acc: 0.91)\n\n#### Example 2\n- **Clean:** `first i think we should rent bigger halls so that we can make a better sound and give more space for the audience`\n- **Noisy:** `first i think we shoul rent bigger halls so that we can make a better sound and give more space for the audience`\n- **My Corrected 1:** `first i think we should rent bigger halls so that we can make a better sound and give more space for the audience` (Acc: 1.00)\n- **My Corrected 2:** `first i think we shout rent bigger halls so that we can make a better sound and give more space for the audience` (Acc: 0.96)\n- **My Corrected (No Keyboard):** `first i think we shout rent bigger halls so that we can make a better sound and give more space for the audience` (Acc: 0.96)\n- **My Corrected 3:** `first i think we shout rent bigger halls so that we can make a better sound and give more space for the audience` (Acc: 0.96)\n- **Norvig Corrected:** `first i think we should rent bigger halls so that we can make a better sound and give more space for the audience` (Acc: 1.00)\n\n#### Example 3\n- **Clean:** `it s a great opportunity for me to participate in your camp california because normally i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children`\n- **Noisy:** `it s a great opportunity for me to participate in your camp california because normaly i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children`\n- **My Corrected 1:** `it s a great opportunity for me to participate in your camp california because normally i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children` (Acc: 1.00)\n- **My Corrected 2:** `it s a great opportunity for me to participate in your camp california because normally i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children` (Acc: 1.00)\n- **My Corrected (No Keyboard):** `it s a great opportunity for me to participate in your camp california because normally i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children` (Acc: 1.00)\n- **My Corrected 3:** `it s a great opportunity for me to participate in your camp california because normally i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children` (Acc: 1.00)\n- **Norvig Corrected:** `it s a great opportunity for me to participate in your camp california because normal i work a lot and i ca n t spend money on travel moreover i have to support a big family because i m married and i have three children` (Acc: 0.98)\n\n### Analysis\n\n#### Overall Performance\n- **Correction 1 Leads:** `Correction 1` achieves the highest average accuracy (0.964859), outperforming all other methods, including Norvig’s solution (0.960724). This suggests that its combination of bigram context and keyboard-aware edits is particularly effective for the human-like errors in the BEA-4K dataset.\n- **Close Competition:** `Correction 2`, `Correction 2 (No Keyboard)`, and `Correction 3` have similar accuracies (0.962504, 0.962504, and 0.962402, respectively), slightly below `Correction 1` but still competitive with Norvig’s 0.960724. This indicates that higher N-gram models (used in `Correction 2` and `3`) provide marginal benefits over Norvig’s unigram approach but may not fully leverage the dataset’s error patterns.\n- **Norvig’s Baseline:** Norvig’s accuracy (0.960724) is the lowest, though still strong, reflecting its reliance on unigram probabilities without context or keyboard-awareness, which limits its ability to handle certain errors effectively.\n\n#### Sample-Specific Insights\n1. **Example 1 (choise → choice):**\n   - All my methods correctly fix \"choise\" to \"choice\" and adjust \"preferences\" to \"preference\" but incorrectly change \"filmed\" to \"filled\" (likely due to N-gram weighting favoring \"filled\" in some contexts). Accuracy is 0.94 for all my methods.\n   - Norvig fails on \"choise\" (outputs \"choose\") and \"preferences\" (outputs \"references\"), resulting in a lower accuracy of 0.91. This highlights the advantage of context-awareness in my methods.\n\n2. **Example 2 (shoul → should):**\n   - `Correction 1` and Norvig perfectly correct \"shoul\" to \"should\" (Acc: 1.00), showing their robustness for simple typos.\n   - `Correction 2`, `Correction 2 (No Keyboard)`, and `Correction 3` incorrectly output \"shout\" (Acc: 0.96), possibly due to over-reliance on higher N-grams or insufficient keyboard context to disambiguate \"shoul\". This suggests a potential weighting issue in these methods for this specific case.\n\n3. **Example 3 (normaly → normally):**\n   - All my methods perfectly correct \"normaly\" to \"normally\" (Acc: 1.00), demonstrating consistent performance on straightforward insertion errors.\n   - Norvig outputs \"normal\" (Acc: 0.98), missing the full correction, likely because \"normal\" has a higher unigram probability than \"normally\" without context to guide it.\n\n#### Keyboard vs. No Keyboard\n- Surprisingly, `Correction 2` and `Correction 2 (No Keyboard)` have identical average accuracies (0.962504), despite BEA-4K containing human-like typos. This could indicate:\n  - The keyboard-adjacent edit feature isn’t being fully utilized, possibly due to implementation details (e.g., limited keyboard edit candidates or insufficient weighting).\n  - The dataset’s errors may not predominantly involve keyboard adjacency (e.g., more insertions like \"normaly\" than swaps like \"teh\"), reducing the feature’s impact.\n- Sample outputs (e.g., \"shoul\" → \"should\" vs. \"shout\") suggest some differentiation, but the overall effect is neutralized across the dataset.\n\n#### Strengths and Weaknesses\n- **Strengths:**\n  - `Correction 1` excels due to its balanced use of bigrams and keyboard edits, making it the most robust for this dataset.\n  - All my methods outperform Norvig on average, validating the value of context (N-grams) over unigrams alone.\n- **Weaknesses:**\n  - Higher N-gram methods (`Correction 2` and `3`) occasionally overcorrect (e.g., \"should\" → \"shout\"), suggesting that trigram/fourgram data might be sparse or misweighted in some cases.\n  - The lack of differentiation between keyboard and no-keyboard methods indicates a need to refine the keyboard edit feature or test on a dataset with more adjacency-specific errors.\n\n","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\n<p style=\"font-size: 24px;\">\nBEA-4K test confirms that my algorithms, particularly `Correction 1`, improve upon Norvig’s solution by leveraging context and keyboard-awareness, achieving a peak accuracy of 0.964859 versus Norvig’s 0.960724. However, the identical performance of `Correction 2` and its no-keyboard variant suggests that the keyboard feature’s potential isn’t fully realized, possibly due to the nature of the errors in BEA-4K or implementation limitations. Future work could involve tuning N-gram weights, enhancing keyboard edit candidate generation, or testing on a dataset with more pronounced keyboard-specific typos to further distinguish these methods.\n</p> ","metadata":{}},{"cell_type":"markdown","source":"#### Useful resources (also included in the archive in moodle):\n\n1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)","metadata":{}}]}